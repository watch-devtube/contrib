# Editing guidelines: https://github.com/watch-devtube/contrib/#how-to-edit-video-metadata

category: conference
tags:
    - storage
    - bigdata
    - architecture
title: '#bbuzz 2016: Julien Le Dem -  Efficient Data formats for Analytics with Parquet and Arrow'
language: English
recordingDate: 1465736920
description: "Find more information here: https://berlinbuzzwords.de/session/efficient-data-formats-analytics-parquet-and-arrow\n\nHadoop makes it relatively easy to store petabytes of data. However, storing data is not enough; columnar layouts for storage and in-memory execution allow the analysis of large amounts of data very quickly and efficiently. It provides the ability for multiple applications to share a common data representation and perform operations at full CPU throughput using SIMD and Vectorization. For interoperability, row based encodings - CSV, Thrift, Avro - combined with general purpose compression algorithms - GZip, LZO, Snappy - are common but inefficient. As discussed extensively in the database literature, a columnar layout with statistics and sorting provides vertical and horizontal partitioning, thus keeping IO to a minimum. Additionally a number of key big data technologies have or will soon have in-memory columnar capabilities. This includes Kudu, Ibis and Drill. Sharing a common in-memory columnar representation allows interoperability without the usual cost of serialization.\n\nUnderstanding modern CPU architecture is critical to maximizing processing throughput. We’ll discuss the advantages of columnar layouts in Parquet and Arrow for in-memory processing and data encodings used for storage - dictionary, bit-packing, prefix coding. We’ll dissect and explain the design choices that enable us to achieve all three goals of interoperability, space and query efficiency. In addition, we’ll provide an overview of what’s coming in Parquet and Arrow in the next year."
