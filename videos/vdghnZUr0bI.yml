# Editing guidelines: https://github.com/watch-devtube/contrib/#how-to-edit-video-metadata


tags:
    - storage
    - ml
    - tensorflow
    - testing
    - python
title: 'The Jupyter Notebook as a transparent way to document machine learning model development'
language: English
recordingDate: 1539210284
description: "The Jupyter Notebook as a transparent way to document machine learning model development: A case study from a US defense agency\nCatherine Ordun (Booz Allen Hamilton)\n\nMachine learning is new to many US government agencies. They need to transparently document each step of a model, from data preparation to final model prediction. One US defense agency has used the Jupyter Notebook to document its steps and show results in the model building process for a series of recurrent neural network (RNNs) algorithms. The project was so successful that the team has recommended the Jupyter Notebook to be a key component in model documentation for all government scientists.\n\nCatherine Ordun walks you through a notebook built to test the feasibility of developing multivariate time series models to predict cases of pertussis collected weekly over a 10-year time period. These models were built in Keras with a TensorFlow backend and built in Jupyter in order to transparently show the progress of training and testing for a US defense agency technical approach. The notebook chronicles the teamâ€™s data science workflow, from data acquisition and preprocessing to neural network building to evaluation and final model selection.\n\nThe project used EpiArchive, publicly available weekly time series data from Los Alamos National Laboratory. The team used the Python requests library to call an API response from the EpiArchive database and convert the disease data for a dozen different infectious diseases into a pandas DataFrame. They also used time series weekly NOAA temperature data and precipitation data as multivariate features and converted and normalized the data. For neural network building, the team built a basic ARIMA time series model to predict weekly pertussis cases achieving a mean absolute error of 6.633, in order to establish a baseline. They then built initial LSTM and GRU models, visualizing the training and validation loss in matplotlib and using the Keras callback function to visualize on TensorBoard (outside of the Jupyter Notebook). As the team experimented with adjusting different hyperparameters and layers for the LSTM and GRU (i.e., adding dropout and changing the activation functions, optimizers, and learning rate), they arrived at a set of final models in the notebook. After several more iterations of hyperparameter tuning, they selected a nonstateful LSTM as the final model of one input layer, one layer with 10 units, and two fully connected layers. This model applied a 20% dropout layer, activation was tanh (hyperbolic tangent), and run on 100 epochs with a batch size of 20. The final model achieved a mean absolute error of 0.0896.\n\nSubscribe to O'Reilly on YouTube: http://goo.gl/n3QSYi\n\nFollow O'Reilly on: \nTwitter: http://twitter.com/oreillymedia\nFacebook: http://facebook.com/OReilly\nInstagram: https://www.instagram.com/oreillymedia\nLinkedIn: https://www.linkedin.com/company-beta/8459/"
